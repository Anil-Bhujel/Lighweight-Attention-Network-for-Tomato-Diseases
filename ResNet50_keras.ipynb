{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Input, Add, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D, AveragePooling2D,GlobalAveragePooling2D, MaxPooling2D, GlobalMaxPooling2D\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.utils import layer_utils\n",
    "from keras.utils.data_utils import get_file\n",
    "from tensorflow.keras.applications.imagenet_utils import preprocess_input\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import pydot\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from keras.utils import plot_model\n",
    "from tensorflow.keras.initializers import glorot_uniform\n",
    "import scipy.misc\n",
    "from datetime import datetime\n",
    "from matplotlib.pyplot import imshow\n",
    "%matplotlib inline\n",
    "\n",
    "# import keras.backend as K\n",
    "# K.set_image_data_format('channels_last')\n",
    "# K.set_learning_phase(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identity_block(X, f, filters, stage, block):\n",
    "    \"\"\"\n",
    "    Implementation of the identity block as defined in Figure 3\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input tensor of shape (m, n_H_prev, n_W_prev, n_C_prev)\n",
    "    f -- integer, specifying the shape of the middle CONV's window for the main path\n",
    "    filters -- python list of integers, defining the number of filters in the CONV layers of the main path\n",
    "    stage -- integer, used to name the layers, depending on their position in the network\n",
    "    block -- string/character, used to name the layers, depending on their position in the network\n",
    "    \n",
    "    Returns:\n",
    "    X -- output of the identity block, tensor of shape (n_H, n_W, n_C)\n",
    "    \"\"\"\n",
    "    \n",
    "    # defining name basis\n",
    "    conv_name_base = 'res' + str(stage) + block + '_branch'\n",
    "    bn_name_base = 'bn' + str(stage) + block + '_branch'\n",
    "    \n",
    "    # Retrieve Filters\n",
    "    F1, F2, F3 = filters\n",
    "    \n",
    "    # Save the input value. You'll need this later to add back to the main path. \n",
    "    X_shortcut = X\n",
    "    \n",
    "    # First component of main path\n",
    "    X = Conv2D(filters = F1, kernel_size = (1, 1), strides = (1,1), padding = 'valid', name = conv_name_base + '2a', kernel_initializer = glorot_uniform(seed=0))(X)\n",
    "    X = BatchNormalization(axis = 3, name = bn_name_base + '2a')(X)\n",
    "    X = Activation('relu')(X)\n",
    "\n",
    "    \n",
    "    # Second component of main path (≈3 lines)\n",
    "    X = Conv2D(filters = F2, kernel_size = (f, f), strides = (1,1), padding = 'same', name = conv_name_base + '2b', kernel_initializer = glorot_uniform(seed=0))(X)\n",
    "    X = BatchNormalization(axis = 3, name = bn_name_base + '2b')(X)\n",
    "    X = Activation('relu')(X)\n",
    "\n",
    "    # Third component of main path (≈2 lines)\n",
    "    X = Conv2D(filters = F3, kernel_size = (1, 1), strides = (1,1), padding = 'valid', name = conv_name_base + '2c', kernel_initializer = glorot_uniform(seed=0))(X)\n",
    "    X = BatchNormalization(axis = 3, name = bn_name_base + '2c')(X)\n",
    "\n",
    "    # Final step: Add shortcut value to main path, and pass it through a RELU activation (≈2 lines)\n",
    "    X = Add()([X, X_shortcut])\n",
    "    X = Activation('relu')(X)\n",
    "    \n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convolutional_block(X, f, filters, stage, block, s = 2):\n",
    "    \"\"\"\n",
    "    Implementation of the convolutional block as defined in Figure 4\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input tensor of shape (m, n_H_prev, n_W_prev, n_C_prev)\n",
    "    f -- integer, specifying the shape of the middle CONV's window for the main path\n",
    "    filters -- python list of integers, defining the number of filters in the CONV layers of the main path\n",
    "    stage -- integer, used to name the layers, depending on their position in the network\n",
    "    block -- string/character, used to name the layers, depending on their position in the network\n",
    "    s -- Integer, specifying the stride to be used\n",
    "    \n",
    "    Returns:\n",
    "    X -- output of the convolutional block, tensor of shape (n_H, n_W, n_C)\n",
    "    \"\"\"\n",
    "    \n",
    "    # defining name basis\n",
    "    conv_name_base = 'res' + str(stage) + block + '_branch'\n",
    "    bn_name_base = 'bn' + str(stage) + block + '_branch'\n",
    "    \n",
    "    # Retrieve Filters\n",
    "    F1, F2, F3 = filters\n",
    "    \n",
    "    # Save the input value\n",
    "    X_shortcut = X\n",
    "\n",
    "\n",
    "    ##### MAIN PATH #####\n",
    "    # First component of main path \n",
    "    X = Conv2D(F1, (1, 1), strides = (s,s), name = conv_name_base + '2a', kernel_initializer = glorot_uniform(seed=0))(X)\n",
    "    X = BatchNormalization(axis = 3, name = bn_name_base + '2a')(X)\n",
    "    X = Activation('relu')(X)\n",
    "\n",
    "    # Second component of main path (≈3 lines)\n",
    "    X = Conv2D(filters = F2, kernel_size = (f, f), strides = (1,1), padding = 'same', name = conv_name_base + '2b', kernel_initializer = glorot_uniform(seed=0))(X)\n",
    "    X = BatchNormalization(axis = 3, name = bn_name_base + '2b')(X)\n",
    "    X = Activation('relu')(X)\n",
    "\n",
    "\n",
    "    # Third component of main path (≈2 lines)\n",
    "    X = Conv2D(filters = F3, kernel_size = (1, 1), strides = (1,1), padding = 'valid', name = conv_name_base + '2c', kernel_initializer = glorot_uniform(seed=0))(X)\n",
    "    X = BatchNormalization(axis = 3, name = bn_name_base + '2c')(X)\n",
    "\n",
    "\n",
    "    ##### SHORTCUT PATH #### (≈2 lines)\n",
    "    X_shortcut = Conv2D(filters = F3, kernel_size = (1, 1), strides = (s,s), padding = 'valid', name = conv_name_base + '1',\n",
    "                        kernel_initializer = glorot_uniform(seed=0))(X_shortcut)\n",
    "    X_shortcut = BatchNormalization(axis = 3, name = bn_name_base + '1')(X_shortcut)\n",
    "\n",
    "    # Final step: Add shortcut value to main path, and pass it through a RELU activation (≈2 lines)\n",
    "    X = Add()([X, X_shortcut])\n",
    "    X = Activation('relu')(X)\n",
    "    \n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ResNet50(input_shape=(64, 64, 3), classes=6):\n",
    "    \"\"\"\n",
    "    Implementation of the popular ResNet50 the following architecture:\n",
    "    CONV2D -> BATCHNORM -> RELU -> MAXPOOL -> CONVBLOCK -> IDBLOCK*2 -> CONVBLOCK -> IDBLOCK*3\n",
    "    -> CONVBLOCK -> IDBLOCK*5 -> CONVBLOCK -> IDBLOCK*2 -> AVGPOOL -> TOPLAYER\n",
    "\n",
    "    Arguments:\n",
    "    input_shape -- shape of the images of the dataset\n",
    "    classes -- integer, number of classes\n",
    "\n",
    "    Returns:\n",
    "    model -- a Model() instance in Keras\n",
    "    \"\"\"\n",
    "\n",
    "    # Define the input as a tensor with shape input_shape\n",
    "    X_input = Input(input_shape)\n",
    "\n",
    "    # Zero-Padding\n",
    "    X = ZeroPadding2D((3, 3))(X_input)\n",
    "\n",
    "    # Stage 1\n",
    "    X = Conv2D(64, (7, 7), strides=(2, 2), name='conv1', kernel_initializer=glorot_uniform(seed=0))(X)\n",
    "    X = BatchNormalization(axis=3, name='bn_conv1')(X)\n",
    "    X = Activation('relu')(X)\n",
    "    X = MaxPooling2D((3, 3), strides=(2, 2))(X)\n",
    "\n",
    "    # Stage 2\n",
    "    X = convolutional_block(X, f=3, filters=[64, 64, 256], stage=2, block='a', s=1)\n",
    "    X = identity_block(X, 3, [64, 64, 256], stage=2, block='b')\n",
    "    X = identity_block(X, 3, [64, 64, 256], stage=2, block='c')\n",
    "\n",
    "    ### START CODE HERE ###\n",
    "\n",
    "    # Stage 3 (≈4 lines)\n",
    "    X = convolutional_block(X, f = 3, filters = [128, 128, 512], stage = 3, block='a', s = 2)\n",
    "    X = identity_block(X, 3, [128, 128, 512], stage=3, block='b')\n",
    "    X = identity_block(X, 3, [128, 128, 512], stage=3, block='c')\n",
    "    X = identity_block(X, 3, [128, 128, 512], stage=3, block='d')\n",
    "\n",
    "    # Stage 4 (≈6 lines)\n",
    "    X = convolutional_block(X, f = 3, filters = [256, 256, 1024], stage = 4, block='a', s = 2)\n",
    "    X = identity_block(X, 3, [256, 256, 1024], stage=4, block='b')\n",
    "    X = identity_block(X, 3, [256, 256, 1024], stage=4, block='c')\n",
    "    X = identity_block(X, 3, [256, 256, 1024], stage=4, block='d')\n",
    "    X = identity_block(X, 3, [256, 256, 1024], stage=4, block='e')\n",
    "    X = identity_block(X, 3, [256, 256, 1024], stage=4, block='f')\n",
    "\n",
    "    # Stage 5 (≈3 lines)\n",
    "    X = convolutional_block(X, f = 3, filters = [512, 512, 2048], stage = 5, block='a', s = 2)\n",
    "    X = identity_block(X, 3, [512, 512, 2048], stage=5, block='b')\n",
    "    X = identity_block(X, 3, [512, 512, 2048], stage=5, block='c')\n",
    "\n",
    "    # AVGPOOL (≈1 line). Use \"X = AveragePooling2D(...)(X)\"\n",
    "    #X = AveragePooling2D((2,2), name=\"avg_pool\")(X)\n",
    "    X = GlobalAveragePooling2D(name=\"avg_pool\")(X)\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    # output layer\n",
    "    #X = Flatten()(X)\n",
    "    X = Dense(classes, activation='softmax', name='fc' + str(classes), kernel_initializer = glorot_uniform(seed=0))(X)\n",
    "    \n",
    "    \n",
    "    # Create model\n",
    "    model = Model(inputs = X_input, outputs = X, name='ResNet50')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"ResNet50\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 256, 256, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d (ZeroPadding2D)  (None, 262, 262, 3)  0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1 (Conv2D)                  (None, 128, 128, 64) 9472        zero_padding2d[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn_conv1 (BatchNormalization)   (None, 128, 128, 64) 256         conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 128, 128, 64) 0           bn_conv1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D)    (None, 63, 63, 64)   0           activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "res2a_branch2a (Conv2D)         (None, 63, 63, 64)   4160        max_pooling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn2a_branch2a (BatchNormalizati (None, 63, 63, 64)   256         res2a_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 63, 63, 64)   0           bn2a_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2a_branch2b (Conv2D)         (None, 63, 63, 64)   36928       activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bn2a_branch2b (BatchNormalizati (None, 63, 63, 64)   256         res2a_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 63, 63, 64)   0           bn2a_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2a_branch2c (Conv2D)         (None, 63, 63, 256)  16640       activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "res2a_branch1 (Conv2D)          (None, 63, 63, 256)  16640       max_pooling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn2a_branch2c (BatchNormalizati (None, 63, 63, 256)  1024        res2a_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn2a_branch1 (BatchNormalizatio (None, 63, 63, 256)  1024        res2a_branch1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 63, 63, 256)  0           bn2a_branch2c[0][0]              \n",
      "                                                                 bn2a_branch1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 63, 63, 256)  0           add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "res2b_branch2a (Conv2D)         (None, 63, 63, 64)   16448       activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bn2b_branch2a (BatchNormalizati (None, 63, 63, 64)   256         res2b_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 63, 63, 64)   0           bn2b_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2b_branch2b (Conv2D)         (None, 63, 63, 64)   36928       activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bn2b_branch2b (BatchNormalizati (None, 63, 63, 64)   256         res2b_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 63, 63, 64)   0           bn2b_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2b_branch2c (Conv2D)         (None, 63, 63, 256)  16640       activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bn2b_branch2c (BatchNormalizati (None, 63, 63, 256)  1024        res2b_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 63, 63, 256)  0           bn2b_branch2c[0][0]              \n",
      "                                                                 activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 63, 63, 256)  0           add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res2c_branch2a (Conv2D)         (None, 63, 63, 64)   16448       activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bn2c_branch2a (BatchNormalizati (None, 63, 63, 64)   256         res2c_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 63, 63, 64)   0           bn2c_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2c_branch2b (Conv2D)         (None, 63, 63, 64)   36928       activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bn2c_branch2b (BatchNormalizati (None, 63, 63, 64)   256         res2c_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 63, 63, 64)   0           bn2c_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2c_branch2c (Conv2D)         (None, 63, 63, 256)  16640       activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bn2c_branch2c (BatchNormalizati (None, 63, 63, 256)  1024        res2c_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 63, 63, 256)  0           bn2c_branch2c[0][0]              \n",
      "                                                                 activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 63, 63, 256)  0           add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res3a_branch2a (Conv2D)         (None, 32, 32, 128)  32896       activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bn3a_branch2a (BatchNormalizati (None, 32, 32, 128)  512         res3a_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 32, 32, 128)  0           bn3a_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3a_branch2b (Conv2D)         (None, 32, 32, 128)  147584      activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3a_branch2b (BatchNormalizati (None, 32, 32, 128)  512         res3a_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 32, 32, 128)  0           bn3a_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3a_branch2c (Conv2D)         (None, 32, 32, 512)  66048       activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3a_branch1 (Conv2D)          (None, 32, 32, 512)  131584      activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bn3a_branch2c (BatchNormalizati (None, 32, 32, 512)  2048        res3a_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn3a_branch1 (BatchNormalizatio (None, 32, 32, 512)  2048        res3a_branch1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 32, 32, 512)  0           bn3a_branch2c[0][0]              \n",
      "                                                                 bn3a_branch1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 32, 32, 512)  0           add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res3b_branch2a (Conv2D)         (None, 32, 32, 128)  65664       activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3b_branch2a (BatchNormalizati (None, 32, 32, 128)  512         res3b_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 32, 32, 128)  0           bn3b_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3b_branch2b (Conv2D)         (None, 32, 32, 128)  147584      activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3b_branch2b (BatchNormalizati (None, 32, 32, 128)  512         res3b_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 32, 32, 128)  0           bn3b_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3b_branch2c (Conv2D)         (None, 32, 32, 512)  66048       activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3b_branch2c (BatchNormalizati (None, 32, 32, 512)  2048        res3b_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 32, 32, 512)  0           bn3b_branch2c[0][0]              \n",
      "                                                                 activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 32, 32, 512)  0           add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res3c_branch2a (Conv2D)         (None, 32, 32, 128)  65664       activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3c_branch2a (BatchNormalizati (None, 32, 32, 128)  512         res3c_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 32, 32, 128)  0           bn3c_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3c_branch2b (Conv2D)         (None, 32, 32, 128)  147584      activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3c_branch2b (BatchNormalizati (None, 32, 32, 128)  512         res3c_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 32, 32, 128)  0           bn3c_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3c_branch2c (Conv2D)         (None, 32, 32, 512)  66048       activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3c_branch2c (BatchNormalizati (None, 32, 32, 512)  2048        res3c_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 32, 32, 512)  0           bn3c_branch2c[0][0]              \n",
      "                                                                 activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 32, 32, 512)  0           add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res3d_branch2a (Conv2D)         (None, 32, 32, 128)  65664       activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3d_branch2a (BatchNormalizati (None, 32, 32, 128)  512         res3d_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 32, 32, 128)  0           bn3d_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3d_branch2b (Conv2D)         (None, 32, 32, 128)  147584      activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3d_branch2b (BatchNormalizati (None, 32, 32, 128)  512         res3d_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 32, 32, 128)  0           bn3d_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3d_branch2c (Conv2D)         (None, 32, 32, 512)  66048       activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3d_branch2c (BatchNormalizati (None, 32, 32, 512)  2048        res3d_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (None, 32, 32, 512)  0           bn3d_branch2c[0][0]              \n",
      "                                                                 activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 32, 32, 512)  0           add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res4a_branch2a (Conv2D)         (None, 16, 16, 256)  131328      activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4a_branch2a (BatchNormalizati (None, 16, 16, 256)  1024        res4a_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 16, 16, 256)  0           bn4a_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4a_branch2b (Conv2D)         (None, 16, 16, 256)  590080      activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4a_branch2b (BatchNormalizati (None, 16, 16, 256)  1024        res4a_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 16, 16, 256)  0           bn4a_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4a_branch2c (Conv2D)         (None, 16, 16, 1024) 263168      activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4a_branch1 (Conv2D)          (None, 16, 16, 1024) 525312      activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4a_branch2c (BatchNormalizati (None, 16, 16, 1024) 4096        res4a_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4a_branch1 (BatchNormalizatio (None, 16, 16, 1024) 4096        res4a_branch1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_7 (Add)                     (None, 16, 16, 1024) 0           bn4a_branch2c[0][0]              \n",
      "                                                                 bn4a_branch1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 16, 16, 1024) 0           add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res4b_branch2a (Conv2D)         (None, 16, 16, 256)  262400      activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4b_branch2a (BatchNormalizati (None, 16, 16, 256)  1024        res4b_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 16, 16, 256)  0           bn4b_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4b_branch2b (Conv2D)         (None, 16, 16, 256)  590080      activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4b_branch2b (BatchNormalizati (None, 16, 16, 256)  1024        res4b_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 16, 16, 256)  0           bn4b_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4b_branch2c (Conv2D)         (None, 16, 16, 1024) 263168      activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4b_branch2c (BatchNormalizati (None, 16, 16, 1024) 4096        res4b_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_8 (Add)                     (None, 16, 16, 1024) 0           bn4b_branch2c[0][0]              \n",
      "                                                                 activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 16, 16, 1024) 0           add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res4c_branch2a (Conv2D)         (None, 16, 16, 256)  262400      activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4c_branch2a (BatchNormalizati (None, 16, 16, 256)  1024        res4c_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 16, 16, 256)  0           bn4c_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4c_branch2b (Conv2D)         (None, 16, 16, 256)  590080      activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4c_branch2b (BatchNormalizati (None, 16, 16, 256)  1024        res4c_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 16, 16, 256)  0           bn4c_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4c_branch2c (Conv2D)         (None, 16, 16, 1024) 263168      activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4c_branch2c (BatchNormalizati (None, 16, 16, 1024) 4096        res4c_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_9 (Add)                     (None, 16, 16, 1024) 0           bn4c_branch2c[0][0]              \n",
      "                                                                 activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 16, 16, 1024) 0           add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res4d_branch2a (Conv2D)         (None, 16, 16, 256)  262400      activation_30[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4d_branch2a (BatchNormalizati (None, 16, 16, 256)  1024        res4d_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 16, 16, 256)  0           bn4d_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4d_branch2b (Conv2D)         (None, 16, 16, 256)  590080      activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4d_branch2b (BatchNormalizati (None, 16, 16, 256)  1024        res4d_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, 16, 16, 256)  0           bn4d_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4d_branch2c (Conv2D)         (None, 16, 16, 1024) 263168      activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4d_branch2c (BatchNormalizati (None, 16, 16, 1024) 4096        res4d_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_10 (Add)                    (None, 16, 16, 1024) 0           bn4d_branch2c[0][0]              \n",
      "                                                                 activation_30[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 16, 16, 1024) 0           add_10[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res4e_branch2a (Conv2D)         (None, 16, 16, 256)  262400      activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4e_branch2a (BatchNormalizati (None, 16, 16, 256)  1024        res4e_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 16, 16, 256)  0           bn4e_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4e_branch2b (Conv2D)         (None, 16, 16, 256)  590080      activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4e_branch2b (BatchNormalizati (None, 16, 16, 256)  1024        res4e_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 16, 16, 256)  0           bn4e_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4e_branch2c (Conv2D)         (None, 16, 16, 1024) 263168      activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4e_branch2c (BatchNormalizati (None, 16, 16, 1024) 4096        res4e_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_11 (Add)                    (None, 16, 16, 1024) 0           bn4e_branch2c[0][0]              \n",
      "                                                                 activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 16, 16, 1024) 0           add_11[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res4f_branch2a (Conv2D)         (None, 16, 16, 256)  262400      activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4f_branch2a (BatchNormalizati (None, 16, 16, 256)  1024        res4f_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 16, 16, 256)  0           bn4f_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4f_branch2b (Conv2D)         (None, 16, 16, 256)  590080      activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4f_branch2b (BatchNormalizati (None, 16, 16, 256)  1024        res4f_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 16, 16, 256)  0           bn4f_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4f_branch2c (Conv2D)         (None, 16, 16, 1024) 263168      activation_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4f_branch2c (BatchNormalizati (None, 16, 16, 1024) 4096        res4f_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_12 (Add)                    (None, 16, 16, 1024) 0           bn4f_branch2c[0][0]              \n",
      "                                                                 activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 16, 16, 1024) 0           add_12[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res5a_branch2a (Conv2D)         (None, 8, 8, 512)    524800      activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn5a_branch2a (BatchNormalizati (None, 8, 8, 512)    2048        res5a_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 8, 8, 512)    0           bn5a_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5a_branch2b (Conv2D)         (None, 8, 8, 512)    2359808     activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn5a_branch2b (BatchNormalizati (None, 8, 8, 512)    2048        res5a_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, 8, 8, 512)    0           bn5a_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5a_branch2c (Conv2D)         (None, 8, 8, 2048)   1050624     activation_41[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5a_branch1 (Conv2D)          (None, 8, 8, 2048)   2099200     activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn5a_branch2c (BatchNormalizati (None, 8, 8, 2048)   8192        res5a_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn5a_branch1 (BatchNormalizatio (None, 8, 8, 2048)   8192        res5a_branch1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_13 (Add)                    (None, 8, 8, 2048)   0           bn5a_branch2c[0][0]              \n",
      "                                                                 bn5a_branch1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, 8, 8, 2048)   0           add_13[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res5b_branch2a (Conv2D)         (None, 8, 8, 512)    1049088     activation_42[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn5b_branch2a (BatchNormalizati (None, 8, 8, 512)    2048        res5b_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_43 (Activation)      (None, 8, 8, 512)    0           bn5b_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5b_branch2b (Conv2D)         (None, 8, 8, 512)    2359808     activation_43[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn5b_branch2b (BatchNormalizati (None, 8, 8, 512)    2048        res5b_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_44 (Activation)      (None, 8, 8, 512)    0           bn5b_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5b_branch2c (Conv2D)         (None, 8, 8, 2048)   1050624     activation_44[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn5b_branch2c (BatchNormalizati (None, 8, 8, 2048)   8192        res5b_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_14 (Add)                    (None, 8, 8, 2048)   0           bn5b_branch2c[0][0]              \n",
      "                                                                 activation_42[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_45 (Activation)      (None, 8, 8, 2048)   0           add_14[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res5c_branch2a (Conv2D)         (None, 8, 8, 512)    1049088     activation_45[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn5c_branch2a (BatchNormalizati (None, 8, 8, 512)    2048        res5c_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_46 (Activation)      (None, 8, 8, 512)    0           bn5c_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5c_branch2b (Conv2D)         (None, 8, 8, 512)    2359808     activation_46[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn5c_branch2b (BatchNormalizati (None, 8, 8, 512)    2048        res5c_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_47 (Activation)      (None, 8, 8, 512)    0           bn5c_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5c_branch2c (Conv2D)         (None, 8, 8, 2048)   1050624     activation_47[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn5c_branch2c (BatchNormalizati (None, 8, 8, 2048)   8192        res5c_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_15 (Add)                    (None, 8, 8, 2048)   0           bn5c_branch2c[0][0]              \n",
      "                                                                 activation_45[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_48 (Activation)      (None, 8, 8, 2048)   0           add_15[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "avg_pool (GlobalAveragePooling2 (None, 2048)         0           activation_48[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "fc11 (Dense)                    (None, 11)           22539       avg_pool[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 23,610,251\n",
      "Trainable params: 23,557,131\n",
      "Non-trainable params: 53,120\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = ResNet50(input_shape = (256, 256, 3), classes = 11)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # standard model load from Keras API\n",
    "# from tensorflow.keras.applications import ResNet50\n",
    "# base_model = ResNet50(include_top=True, weights=None)\n",
    "# base_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# w = base_model.layers[10].get_weights()[0]\n",
    "# w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x=base_model.layers[-2].output\n",
    "# predictions = Dense(11,activation='softmax')(x)\n",
    "# model = Model(inputs=base_model.input,outputs=predictions)\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # standard model load from Keras API\n",
    "# from tensorflow.keras.applications import ResNet50\n",
    "# base_model = ResNet50(include_top=False, input_shape= (256,256,3),weights=None)\n",
    "# base_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # add a global spatial average pooling layer\n",
    "# x = base_model.output\n",
    "# x = GlobalAveragePooling2D(name=\"avg_pool\")(x)\n",
    "\n",
    "# predictions = Dense(11, activation='softmax')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # this is the model we will train\n",
    "# model = Model(inputs=base_model.input, outputs=predictions)\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # standard model load from Keras API\n",
    "# from tensorflow.keras.applications import ResNet50V2\n",
    "# base_modelV2 = ResNet50V2(include_top=False, weights=None)\n",
    "# base_modelV2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', \n",
    "              loss='categorical_crossentropy', \n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 15156 images belonging to 11 classes.\n",
      "Found 1891 images belonging to 11 classes.\n",
      "{'bacterial_spot': 0, 'early_blight': 1, 'fusarium_wilt': 2, 'healthy': 3, 'late_blight': 4, 'leaf_mold': 5, 'mosaic_virus': 6, 'septoria_leaf_spot': 7, 'spider_mites': 8, 'target_spot': 9, 'yellow_leaf_curl_virus': 10}\n"
     ]
    }
   ],
   "source": [
    "# image preprocessing\n",
    "train_datagen = ImageDataGenerator(rescale=1./255,\n",
    "                                   shear_range=0.2,\n",
    "                                   zoom_range=0.2,\n",
    "                                   width_shift_range=0.2,\n",
    "                                   height_shift_range=0.2,\n",
    "                                   rotation_range=40,\n",
    "                                   horizontal_flip=True,\n",
    "                                   vertical_flip=True,\n",
    "                                   fill_mode='nearest')\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "train_data_dir = \"D:/Anil/Dataset/tomato/train\"     # directory of training data\n",
    "\n",
    "test_data_dir = \"D:/Anil/Dataset/tomato/val\"      # directory of Validation data\n",
    "\n",
    "training_set = train_datagen.flow_from_directory(train_data_dir,\n",
    "                                                 target_size=(256, 256),\n",
    "                                                 batch_size=batch_size,\n",
    "                                                 class_mode='categorical')\n",
    "\n",
    "test_set = test_datagen.flow_from_directory(test_data_dir,\n",
    "                                            target_size=(256, 256),\n",
    "                                            batch_size=batch_size,\n",
    "                                            class_mode='categorical')\n",
    "\n",
    "print(training_set.class_indices)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start time: 2021-12-20 19:01:46.849819\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001FB5AAB8438> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001FB5AAB8438> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "473/473 [==============================] - ETA: 0s - loss: 1.7087 - accuracy: 0.5434WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001FD2371EA68> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001FD2371EA68> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "473/473 [==============================] - 145s 292ms/step - loss: 1.7076 - accuracy: 0.5436 - val_loss: 12.2500 - val_accuracy: 0.2807\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 12.24997, saving model to CBAM-keras/saved_models\\ResNet50_tomato_adam_100_epochs.hdf5\n",
      "Epoch 2/100\n",
      "473/473 [==============================] - 137s 289ms/step - loss: 0.7139 - accuracy: 0.7567 - val_loss: 0.8666 - val_accuracy: 0.6917\n",
      "\n",
      "Epoch 00002: val_loss improved from 12.24997 to 0.86663, saving model to CBAM-keras/saved_models\\ResNet50_tomato_adam_100_epochs.hdf5\n",
      "Epoch 3/100\n",
      "473/473 [==============================] - 136s 288ms/step - loss: 0.5651 - accuracy: 0.8103 - val_loss: 14.2879 - val_accuracy: 0.0959\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.86663\n",
      "Epoch 4/100\n",
      "473/473 [==============================] - 136s 287ms/step - loss: 0.5651 - accuracy: 0.8112 - val_loss: 0.8020 - val_accuracy: 0.7209\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.86663 to 0.80202, saving model to CBAM-keras/saved_models\\ResNet50_tomato_adam_100_epochs.hdf5\n",
      "Epoch 5/100\n",
      "473/473 [==============================] - 136s 287ms/step - loss: 0.4302 - accuracy: 0.8522 - val_loss: 0.7612 - val_accuracy: 0.8570\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.80202 to 0.76123, saving model to CBAM-keras/saved_models\\ResNet50_tomato_adam_100_epochs.hdf5\n",
      "Epoch 6/100\n",
      "473/473 [==============================] - 136s 287ms/step - loss: 0.4062 - accuracy: 0.8633 - val_loss: 5.3434 - val_accuracy: 0.2950\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.76123\n",
      "Epoch 7/100\n",
      "473/473 [==============================] - 136s 287ms/step - loss: 0.3410 - accuracy: 0.8842 - val_loss: 0.9926 - val_accuracy: 0.7214\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.76123\n",
      "Epoch 8/100\n",
      "473/473 [==============================] - 136s 287ms/step - loss: 0.3889 - accuracy: 0.8729 - val_loss: 0.8939 - val_accuracy: 0.7214\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.76123\n",
      "Epoch 9/100\n",
      "473/473 [==============================] - 136s 287ms/step - loss: 0.3496 - accuracy: 0.8831 - val_loss: 5.5625 - val_accuracy: 0.4280\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.76123\n",
      "Epoch 10/100\n",
      "473/473 [==============================] - 136s 287ms/step - loss: 0.2964 - accuracy: 0.9015 - val_loss: 0.3344 - val_accuracy: 0.8967\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.76123 to 0.33442, saving model to CBAM-keras/saved_models\\ResNet50_tomato_adam_100_epochs.hdf5\n",
      "Epoch 11/100\n",
      "473/473 [==============================] - 136s 286ms/step - loss: 0.2313 - accuracy: 0.9201 - val_loss: 0.2851 - val_accuracy: 0.9057\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.33442 to 0.28506, saving model to CBAM-keras/saved_models\\ResNet50_tomato_adam_100_epochs.hdf5\n",
      "Epoch 12/100\n",
      "473/473 [==============================] - 136s 287ms/step - loss: 0.2130 - accuracy: 0.9271 - val_loss: 1.6285 - val_accuracy: 0.7002\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.28506\n",
      "Epoch 13/100\n",
      "473/473 [==============================] - 136s 287ms/step - loss: 0.2346 - accuracy: 0.9187 - val_loss: 0.6301 - val_accuracy: 0.8263\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.28506\n",
      "Epoch 14/100\n",
      "473/473 [==============================] - 136s 287ms/step - loss: 0.2168 - accuracy: 0.9245 - val_loss: 0.6283 - val_accuracy: 0.7903\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.28506\n",
      "Epoch 15/100\n",
      "473/473 [==============================] - 136s 287ms/step - loss: 0.1478 - accuracy: 0.9500 - val_loss: 0.4290 - val_accuracy: 0.8591\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.28506\n",
      "Epoch 16/100\n",
      "473/473 [==============================] - 136s 287ms/step - loss: 0.1534 - accuracy: 0.9480 - val_loss: 0.7299 - val_accuracy: 0.7860\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.28506\n",
      "Epoch 17/100\n",
      "473/473 [==============================] - 136s 287ms/step - loss: 0.1613 - accuracy: 0.9443 - val_loss: 3.5994 - val_accuracy: 0.4677\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.28506\n",
      "Epoch 18/100\n",
      "473/473 [==============================] - 136s 286ms/step - loss: 0.1430 - accuracy: 0.9517 - val_loss: 0.1992 - val_accuracy: 0.9375\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.28506 to 0.19924, saving model to CBAM-keras/saved_models\\ResNet50_tomato_adam_100_epochs.hdf5\n",
      "Epoch 19/100\n",
      "473/473 [==============================] - 135s 286ms/step - loss: 0.1424 - accuracy: 0.9513 - val_loss: 1.8316 - val_accuracy: 0.6536\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.19924\n",
      "Epoch 20/100\n",
      "473/473 [==============================] - 136s 287ms/step - loss: 0.1356 - accuracy: 0.9543 - val_loss: 0.5074 - val_accuracy: 0.8390\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.19924\n",
      "Epoch 21/100\n",
      "473/473 [==============================] - 136s 287ms/step - loss: 0.1243 - accuracy: 0.9577 - val_loss: 1.5422 - val_accuracy: 0.7675\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.19924\n",
      "Epoch 22/100\n",
      "473/473 [==============================] - 136s 287ms/step - loss: 0.1443 - accuracy: 0.9551 - val_loss: 2.9614 - val_accuracy: 0.5969\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.19924\n",
      "Epoch 23/100\n",
      "473/473 [==============================] - 136s 286ms/step - loss: 0.1184 - accuracy: 0.9588 - val_loss: 0.9414 - val_accuracy: 0.7918\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.19924\n",
      "Epoch 24/100\n",
      "473/473 [==============================] - 136s 286ms/step - loss: 0.1132 - accuracy: 0.9594 - val_loss: 0.0975 - val_accuracy: 0.9656\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.19924 to 0.09749, saving model to CBAM-keras/saved_models\\ResNet50_tomato_adam_100_epochs.hdf5\n",
      "Epoch 25/100\n",
      "473/473 [==============================] - 136s 287ms/step - loss: 0.1061 - accuracy: 0.9638 - val_loss: 0.6683 - val_accuracy: 0.8464\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.09749\n",
      "Epoch 26/100\n",
      "473/473 [==============================] - 136s 286ms/step - loss: 0.0931 - accuracy: 0.9690 - val_loss: 5.2423 - val_accuracy: 0.5519\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.09749\n",
      "Epoch 27/100\n",
      "473/473 [==============================] - 136s 287ms/step - loss: 0.0875 - accuracy: 0.9699 - val_loss: 0.3778 - val_accuracy: 0.9142\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.09749\n",
      "Epoch 28/100\n",
      "473/473 [==============================] - 136s 286ms/step - loss: 0.1005 - accuracy: 0.9661 - val_loss: 0.1853 - val_accuracy: 0.9396\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.09749\n",
      "Epoch 29/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "473/473 [==============================] - 136s 286ms/step - loss: 0.0860 - accuracy: 0.9724 - val_loss: 0.0902 - val_accuracy: 0.9698\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.09749 to 0.09018, saving model to CBAM-keras/saved_models\\ResNet50_tomato_adam_100_epochs.hdf5\n",
      "Epoch 30/100\n",
      "473/473 [==============================] - 138s 290ms/step - loss: 0.0848 - accuracy: 0.9692 - val_loss: 0.2557 - val_accuracy: 0.9211\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.09018\n",
      "Epoch 31/100\n",
      "473/473 [==============================] - 136s 287ms/step - loss: 0.0727 - accuracy: 0.9742 - val_loss: 0.4121 - val_accuracy: 0.8787\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.09018\n",
      "Epoch 32/100\n",
      "473/473 [==============================] - 136s 286ms/step - loss: 0.0838 - accuracy: 0.9716 - val_loss: 2.3515 - val_accuracy: 0.6679\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.09018\n",
      "Epoch 33/100\n",
      "473/473 [==============================] - 136s 287ms/step - loss: 0.0676 - accuracy: 0.9788 - val_loss: 0.0703 - val_accuracy: 0.9740\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.09018 to 0.07030, saving model to CBAM-keras/saved_models\\ResNet50_tomato_adam_100_epochs.hdf5\n",
      "Epoch 34/100\n",
      "473/473 [==============================] - 136s 287ms/step - loss: 0.0677 - accuracy: 0.9784 - val_loss: 1.3511 - val_accuracy: 0.8072\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.07030\n",
      "Epoch 35/100\n",
      "473/473 [==============================] - 136s 287ms/step - loss: 0.0694 - accuracy: 0.9770 - val_loss: 0.9307 - val_accuracy: 0.8464\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.07030\n",
      "Epoch 36/100\n",
      "473/473 [==============================] - 136s 287ms/step - loss: 0.0639 - accuracy: 0.9776 - val_loss: 0.3957 - val_accuracy: 0.8914\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.07030\n",
      "Epoch 37/100\n",
      "473/473 [==============================] - 136s 287ms/step - loss: 0.0552 - accuracy: 0.9805 - val_loss: 3.6633 - val_accuracy: 0.6340\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.07030\n",
      "Epoch 38/100\n",
      "473/473 [==============================] - 136s 287ms/step - loss: 0.0524 - accuracy: 0.9802 - val_loss: 0.1092 - val_accuracy: 0.9725\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.07030\n",
      "Epoch 39/100\n",
      "473/473 [==============================] - 136s 287ms/step - loss: 0.0732 - accuracy: 0.9775 - val_loss: 0.9767 - val_accuracy: 0.7977\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.07030\n",
      "Epoch 40/100\n",
      "473/473 [==============================] - 136s 286ms/step - loss: 0.0617 - accuracy: 0.9782 - val_loss: 0.5881 - val_accuracy: 0.8469\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.07030\n",
      "Epoch 41/100\n",
      "473/473 [==============================] - 136s 286ms/step - loss: 0.0610 - accuracy: 0.9776 - val_loss: 0.9312 - val_accuracy: 0.8109\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.07030\n",
      "Epoch 42/100\n",
      "473/473 [==============================] - 136s 287ms/step - loss: 0.0569 - accuracy: 0.9811 - val_loss: 0.1043 - val_accuracy: 0.9661\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.07030\n",
      "Epoch 43/100\n",
      "473/473 [==============================] - 136s 286ms/step - loss: 0.0438 - accuracy: 0.9847 - val_loss: 2.1045 - val_accuracy: 0.7685\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.07030\n",
      "Epoch 44/100\n",
      "473/473 [==============================] - 136s 287ms/step - loss: 0.0651 - accuracy: 0.9777 - val_loss: 0.1166 - val_accuracy: 0.9592\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.07030\n",
      "Epoch 45/100\n",
      "473/473 [==============================] - 136s 287ms/step - loss: 0.0823 - accuracy: 0.9735 - val_loss: 0.0389 - val_accuracy: 0.9883\n",
      "\n",
      "Epoch 00045: val_loss improved from 0.07030 to 0.03890, saving model to CBAM-keras/saved_models\\ResNet50_tomato_adam_100_epochs.hdf5\n",
      "Epoch 46/100\n",
      "473/473 [==============================] - 136s 287ms/step - loss: 0.0364 - accuracy: 0.9880 - val_loss: 0.9034 - val_accuracy: 0.8199\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.03890\n",
      "Epoch 47/100\n",
      "473/473 [==============================] - 140s 296ms/step - loss: 0.0419 - accuracy: 0.9859 - val_loss: 0.3924 - val_accuracy: 0.9285\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.03890\n",
      "Epoch 48/100\n",
      "473/473 [==============================] - 139s 294ms/step - loss: 0.0458 - accuracy: 0.9857 - val_loss: 18.5863 - val_accuracy: 0.1954\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.03890\n",
      "Epoch 49/100\n",
      "473/473 [==============================] - 135s 286ms/step - loss: 0.0511 - accuracy: 0.9824 - val_loss: 0.5705 - val_accuracy: 0.8559\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.03890\n",
      "Epoch 50/100\n",
      "473/473 [==============================] - 135s 286ms/step - loss: 0.0389 - accuracy: 0.9861 - val_loss: 0.1244 - val_accuracy: 0.9603\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.03890\n",
      "Epoch 51/100\n",
      "473/473 [==============================] - 136s 286ms/step - loss: 0.0355 - accuracy: 0.9887 - val_loss: 3.0697 - val_accuracy: 0.6785\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.03890\n",
      "Epoch 52/100\n",
      "473/473 [==============================] - 135s 286ms/step - loss: 0.0418 - accuracy: 0.9868 - val_loss: 0.7029 - val_accuracy: 0.8708\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.03890\n",
      "Epoch 53/100\n",
      "473/473 [==============================] - 136s 286ms/step - loss: 0.0390 - accuracy: 0.9861 - val_loss: 0.2336 - val_accuracy: 0.9311\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.03890\n",
      "Epoch 54/100\n",
      "473/473 [==============================] - 136s 287ms/step - loss: 0.0585 - accuracy: 0.9826 - val_loss: 0.2500 - val_accuracy: 0.9216\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.03890\n",
      "Epoch 55/100\n",
      "473/473 [==============================] - 136s 286ms/step - loss: 0.0370 - accuracy: 0.9873 - val_loss: 0.0781 - val_accuracy: 0.9751\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.03890\n",
      "Epoch 56/100\n",
      "473/473 [==============================] - 135s 286ms/step - loss: 0.0333 - accuracy: 0.9881 - val_loss: 0.4887 - val_accuracy: 0.8681\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.03890\n",
      "Epoch 57/100\n",
      "473/473 [==============================] - 136s 286ms/step - loss: 0.0336 - accuracy: 0.9886 - val_loss: 13.2350 - val_accuracy: 0.3374\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.03890\n",
      "Epoch 58/100\n",
      "473/473 [==============================] - 136s 286ms/step - loss: 0.0348 - accuracy: 0.9876 - val_loss: 1.3209 - val_accuracy: 0.8263\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.03890\n",
      "Epoch 59/100\n",
      "473/473 [==============================] - 136s 287ms/step - loss: 0.0329 - accuracy: 0.9892 - val_loss: 1.6257 - val_accuracy: 0.7802\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.03890\n",
      "Epoch 60/100\n",
      "473/473 [==============================] - 136s 287ms/step - loss: 0.0405 - accuracy: 0.9875 - val_loss: 2.7538 - val_accuracy: 0.7278\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.03890\n",
      "Epoch 61/100\n",
      "473/473 [==============================] - 136s 286ms/step - loss: 0.0345 - accuracy: 0.9880 - val_loss: 0.9112 - val_accuracy: 0.8628\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.03890\n",
      "Epoch 62/100\n",
      "473/473 [==============================] - 136s 286ms/step - loss: 0.0486 - accuracy: 0.9839 - val_loss: 0.3114 - val_accuracy: 0.9343\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.03890\n",
      "Epoch 63/100\n",
      "473/473 [==============================] - 136s 286ms/step - loss: 0.0396 - accuracy: 0.9848 - val_loss: 0.7903 - val_accuracy: 0.8618\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.03890\n",
      "Epoch 64/100\n",
      "473/473 [==============================] - 136s 286ms/step - loss: 0.0345 - accuracy: 0.9880 - val_loss: 0.0409 - val_accuracy: 0.9889\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.03890\n",
      "Epoch 65/100\n",
      "473/473 [==============================] - 135s 286ms/step - loss: 0.0286 - accuracy: 0.9899 - val_loss: 1.3097 - val_accuracy: 0.8337\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.03890\n",
      "Epoch 66/100\n",
      "473/473 [==============================] - 136s 286ms/step - loss: 0.0353 - accuracy: 0.9882 - val_loss: 5.3236 - val_accuracy: 0.4968\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.03890\n",
      "Epoch 67/100\n",
      "473/473 [==============================] - 136s 286ms/step - loss: 0.0425 - accuracy: 0.9844 - val_loss: 4.4115 - val_accuracy: 0.6059\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.03890\n",
      "Epoch 68/100\n",
      "473/473 [==============================] - 136s 286ms/step - loss: 0.0311 - accuracy: 0.9892 - val_loss: 0.1991 - val_accuracy: 0.9544\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.03890\n",
      "Epoch 69/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "473/473 [==============================] - 136s 287ms/step - loss: 0.0317 - accuracy: 0.9877 - val_loss: 0.2253 - val_accuracy: 0.9290\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.03890\n",
      "Epoch 70/100\n",
      "473/473 [==============================] - 135s 286ms/step - loss: 0.0291 - accuracy: 0.9898 - val_loss: 2.2479 - val_accuracy: 0.6992\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.03890\n",
      "Epoch 71/100\n",
      "473/473 [==============================] - 135s 286ms/step - loss: 0.0446 - accuracy: 0.9870 - val_loss: 0.7043 - val_accuracy: 0.8739\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.03890\n",
      "Epoch 72/100\n",
      "473/473 [==============================] - 136s 286ms/step - loss: 0.0360 - accuracy: 0.9906 - val_loss: 0.2549 - val_accuracy: 0.9285\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.03890\n",
      "Epoch 73/100\n",
      "473/473 [==============================] - 136s 287ms/step - loss: 0.0291 - accuracy: 0.9911 - val_loss: 7.0847 - val_accuracy: 0.4211\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.03890\n",
      "Epoch 74/100\n",
      "473/473 [==============================] - 136s 287ms/step - loss: 0.0337 - accuracy: 0.9880 - val_loss: 0.8207 - val_accuracy: 0.8586\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.03890\n",
      "Epoch 75/100\n",
      "473/473 [==============================] - 136s 287ms/step - loss: 0.0307 - accuracy: 0.9897 - val_loss: 0.2070 - val_accuracy: 0.9486\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.03890\n",
      "Epoch 76/100\n",
      "473/473 [==============================] - 135s 285ms/step - loss: 0.0297 - accuracy: 0.9905 - val_loss: 1.8676 - val_accuracy: 0.6780\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.03890\n",
      "Epoch 77/100\n",
      "473/473 [==============================] - 136s 286ms/step - loss: 0.0665 - accuracy: 0.9810 - val_loss: 0.0993 - val_accuracy: 0.9740\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.03890\n",
      "Epoch 78/100\n",
      "473/473 [==============================] - 136s 287ms/step - loss: 0.0249 - accuracy: 0.9911 - val_loss: 0.0878 - val_accuracy: 0.9725\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.03890\n",
      "Epoch 79/100\n",
      "473/473 [==============================] - 136s 287ms/step - loss: 0.0216 - accuracy: 0.9926 - val_loss: 1.6360 - val_accuracy: 0.7124\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.03890\n",
      "Epoch 80/100\n",
      "473/473 [==============================] - 136s 286ms/step - loss: 0.0308 - accuracy: 0.9882 - val_loss: 1.6298 - val_accuracy: 0.8268\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.03890\n",
      "Epoch 81/100\n",
      "473/473 [==============================] - 136s 286ms/step - loss: 0.0197 - accuracy: 0.9921 - val_loss: 2.1803 - val_accuracy: 0.7807\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.03890\n",
      "Epoch 82/100\n",
      "473/473 [==============================] - 136s 286ms/step - loss: 0.0235 - accuracy: 0.9911 - val_loss: 0.7145 - val_accuracy: 0.8808\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.03890\n",
      "Epoch 83/100\n",
      "473/473 [==============================] - 136s 288ms/step - loss: 0.0307 - accuracy: 0.9898 - val_loss: 1.1118 - val_accuracy: 0.8056\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.03890\n",
      "Epoch 84/100\n",
      "473/473 [==============================] - 141s 297ms/step - loss: 0.0235 - accuracy: 0.9912 - val_loss: 0.1108 - val_accuracy: 0.9635\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.03890\n",
      "Epoch 85/100\n",
      "473/473 [==============================] - 138s 292ms/step - loss: 0.0267 - accuracy: 0.9900 - val_loss: 0.0212 - val_accuracy: 0.9947\n",
      "\n",
      "Epoch 00085: val_loss improved from 0.03890 to 0.02122, saving model to CBAM-keras/saved_models\\ResNet50_tomato_adam_100_epochs.hdf5\n",
      "Epoch 86/100\n",
      "473/473 [==============================] - 136s 287ms/step - loss: 0.0246 - accuracy: 0.9924 - val_loss: 0.4815 - val_accuracy: 0.9301\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.02122\n",
      "Epoch 87/100\n",
      "473/473 [==============================] - 136s 286ms/step - loss: 0.0262 - accuracy: 0.9915 - val_loss: 0.4080 - val_accuracy: 0.9142\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.02122\n",
      "Epoch 88/100\n",
      "473/473 [==============================] - 136s 286ms/step - loss: 0.0421 - accuracy: 0.9869 - val_loss: 0.4935 - val_accuracy: 0.9020\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.02122\n",
      "Epoch 89/100\n",
      "473/473 [==============================] - 136s 286ms/step - loss: 0.0321 - accuracy: 0.9904 - val_loss: 0.0937 - val_accuracy: 0.9746\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.02122\n",
      "Epoch 90/100\n",
      "473/473 [==============================] - 135s 286ms/step - loss: 0.0296 - accuracy: 0.9910 - val_loss: 0.1185 - val_accuracy: 0.9635\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.02122\n",
      "Epoch 91/100\n",
      "473/473 [==============================] - 136s 286ms/step - loss: 0.0178 - accuracy: 0.9921 - val_loss: 1.0908 - val_accuracy: 0.8935\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.02122\n",
      "Epoch 92/100\n",
      "473/473 [==============================] - 136s 286ms/step - loss: 0.0207 - accuracy: 0.9919 - val_loss: 0.1273 - val_accuracy: 0.9603\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.02122\n",
      "Epoch 93/100\n",
      "473/473 [==============================] - 136s 286ms/step - loss: 0.0205 - accuracy: 0.9936 - val_loss: 0.1842 - val_accuracy: 0.9613\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.02122\n",
      "Epoch 94/100\n",
      "473/473 [==============================] - 136s 286ms/step - loss: 0.0279 - accuracy: 0.9896 - val_loss: 3.2928 - val_accuracy: 0.7516\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.02122\n",
      "Epoch 95/100\n",
      "473/473 [==============================] - 136s 286ms/step - loss: 0.0245 - accuracy: 0.9901 - val_loss: 2.0728 - val_accuracy: 0.7585\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.02122\n",
      "Epoch 96/100\n",
      "473/473 [==============================] - 136s 286ms/step - loss: 0.0351 - accuracy: 0.9886 - val_loss: 0.3106 - val_accuracy: 0.9354\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.02122\n",
      "Epoch 97/100\n",
      "473/473 [==============================] - 136s 286ms/step - loss: 0.0252 - accuracy: 0.9925 - val_loss: 1.0990 - val_accuracy: 0.8120\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.02122\n",
      "Epoch 98/100\n",
      "473/473 [==============================] - 136s 286ms/step - loss: 0.0240 - accuracy: 0.9917 - val_loss: 0.1585 - val_accuracy: 0.9629\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.02122\n",
      "Epoch 99/100\n",
      "473/473 [==============================] - 136s 286ms/step - loss: 0.0225 - accuracy: 0.9912 - val_loss: 0.2354 - val_accuracy: 0.9412\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.02122\n",
      "Epoch 100/100\n",
      "473/473 [==============================] - 137s 288ms/step - loss: 0.0192 - accuracy: 0.9932 - val_loss: 0.0625 - val_accuracy: 0.9820\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.02122\n",
      "stop time: 2021-12-20 22:48:38.772063\n",
      "Total time: 3:46:51.922244\n"
     ]
    }
   ],
   "source": [
    "# checkpoint\n",
    "#weightpath = \"D:/Anil/saved_model/SACNN/ResNet50_tomato_adam_100_epochs.hdf5\"\n",
    "weightpath = \"CBAM-keras/saved_models/ResNet50_tomato_adam_100_epochs.hdf5\"\n",
    "checkpointer = ModelCheckpoint(weightpath, monitor='val_loss', verbose=1, save_best_only=True, mode='auto')\n",
    "\n",
    "epochs = 100\n",
    "\n",
    "start_time = datetime.now()\n",
    "print(\"start time: \"+str(start_time))\n",
    "\n",
    "#fitting images to CNN\n",
    "history = model.fit(training_set,\n",
    "                         steps_per_epoch=training_set.samples//batch_size,\n",
    "                         validation_data=test_set,\n",
    "                         epochs=epochs,\n",
    "                         validation_steps=test_set.samples//batch_size,\n",
    "                         callbacks=[checkpointer])#callbacks_list)\n",
    "\n",
    "\n",
    "# #fitting images to CNN\n",
    "# history = classifier.fit_generator(training_set,\n",
    "#                                    steps_per_epoch=training_set.samples//batch_size,\n",
    "#                                    validation_data=test_set,\n",
    "#                                    epochs=10,\n",
    "#                                    validation_steps=test_set.samples//batch_size)\n",
    "\n",
    "# Stop time\n",
    "stop_time = datetime.now()\n",
    "print(\"stop time: \"+str(stop_time))\n",
    "\n",
    "total_time = stop_time - start_time\n",
    "print(\"Total time: \"+str(total_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# another way to save it.\n",
    "import pandas as pd\n",
    "hist_df = pd.DataFrame(history.history) \n",
    "#hist_csv_file = \"D:/Anil/saved_model/SACNN/history_log_lab_ResNet50_adam_80epochs.csv\"\n",
    "hist_csv_file = \"CBAM-keras/saved_models/ResNet50_tomato_adam_100_epochs.csv\"\n",
    "# hist_csv_file = 'history.csv'\n",
    "with open(hist_csv_file, mode='w', newline='') as f:\n",
    "    hist_df.to_csv(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting training values\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(1, len(loss) + 1)\n",
    "\n",
    "#accuracy plot\n",
    "plt.plot(epochs, acc, color='green', label='Training_Acc')\n",
    "plt.plot(epochs, val_acc, color='blue', label='Validation_Acc')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='best')\n",
    "plt.savefig(\"D:/Anil/saved_model/SACNN/accuracy_graph_tain_val_lab_ResNet_adam_80epochs.png\", bbox_inches=\"tight\", pad_inches=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss plot\n",
    "plt.plot(epochs, loss, color='pink', label='Training Loss')\n",
    "plt.plot(epochs, val_loss, color='red', label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(['Train_loss', 'Val_loss'], loc='best')\n",
    "plt.legend()\n",
    "\n",
    "plt.savefig(\"D:/Anil/saved_model/SACNN/loss_graph_train_val_ResNet50_lab_adam_80epochs.png\", bbox_inches=\"tight\", pad_inches=2)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#loss plot\n",
    "plt.plot(epochs, acc, color='green', label='Training_Acc')\n",
    "plt.plot(epochs, loss, color='red', label='Training_Loss')\n",
    "plt.title('Training Accuracy and Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('ACC&Loss')\n",
    "plt.legend(['Acc', 'Loss'], loc='best')\n",
    "plt.legend()\n",
    "\n",
    "plt.savefig(\"D:/Anil/saved_model/SACNN/train_accuracy_loss_graph_lab_ResNet50_adam_80epochs.png\", bbox_inches=\"tight\", pad_inches=2)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing of model\n",
    "import os\n",
    "test_data_dir = 'D:/Anil/Dataset/tomato/test_imgs'\n",
    "batch_size = 32\n",
    "img_width, img_height = 224, 224\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_image_label(directory):\n",
    "    ''' A generator that yields (label, id, jpg_filename) tuple.'''\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for f in files:\n",
    "            _, ext = os.path.splitext(f)\n",
    "            if ext != '.jpg':\n",
    "                continue\n",
    "            basename = os.path.basename(f)\n",
    "            splits = basename.split('.')\n",
    "            if len(splits) == 3:\n",
    "                label, id_, ext = splits\n",
    "            else:\n",
    "                label = None\n",
    "                id_, ext = splits\n",
    "            fullname = os.path.join(root, f)\n",
    "            yield label, int(id_), fullname\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Wrap testing data into pandas' DataFrame.\n",
    "lst = list(gen_image_label(test_data_dir))\n",
    "test_df = pd.DataFrame(lst, columns=['label', 'id', 'filename'])\n",
    "test_df = test_df.sort_values(by=['label', 'id'])\n",
    "test_df['label_code'] = test_df.label.map({'bacterial_spot': 0, 'early_blight': 1, 'fusarium_wilt': 2, 'healthy': 3, \n",
    "                                           'late_blight': 4, 'leaf_mold': 5, 'mosaic_virus': 6, 'septoria_leaf_spot': 7, \n",
    "                                           'spider_mites': 8, 'target_spot': 9, 'yellow_leaf_curl_virus': 10})\n",
    "\n",
    "test_df.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_true = test_df.label_code\n",
    "print(Y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No need to run again once the file save in computer\n",
    "import numpy as np\n",
    "import cv2\n",
    "img_width = 224\n",
    "img_height = 224\n",
    "images = []\n",
    "dim = (224,224)\n",
    "for img in test_df.filename:\n",
    "#     img = image.load_img(img, target_size=(img_width, img_height))\n",
    "#     img = image.img_to_array(img)\n",
    "    img = cv2.imread(img)\n",
    "    img = cv2.resize(img,dim,interpolation=cv2.INTER_AREA)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2LAB)\n",
    "    img = img/255.0\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    images.append(img)\n",
    "# print(images)\n",
    "# np.save(\"D:/Anil/Dataset/ageng_conf/test_imgs_299x299\", images)\n",
    "# stack up images list to pass for prediction\n",
    "images = np.vstack(images)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"D:/Anil/saved_model/SACNN/ResNet50_tomato_lab_adam_80_epochs.hdf5\"\n",
    "model = load_model(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_start = datetime.now()\n",
    "Y_pred = model.predict(images, batch_size=8)\n",
    "Y_pred_classes = np.argmax(Y_pred, axis = 1)\n",
    "# classes = model.predict_classes(images, batch_size=8)\n",
    "print(Y_pred_classes)\n",
    "test_finish = datetime.now()\n",
    "test_time = test_finish - test_start\n",
    "print(test_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from mlxtend.plotting import plot_confusion_matrix\n",
    "\n",
    "cls_name = ['0','1','2','3','4','5','6','7','8','9','10']\n",
    "\n",
    "ytrue = Y_true.values.flatten()\n",
    "cm = confusion_matrix(ytrue, Y_pred_classes)\n",
    "fig, ax = plot_confusion_matrix(conf_mat=cm,\n",
    "                                figsize = (11,9),\n",
    "                               show_absolute=False,\n",
    "                               show_normed=True,\n",
    "                               colorbar=True,\n",
    "                               class_names = cls_name)\n",
    "plt.show()\n",
    "# print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.savefig(\"D:/Anil/saved_model/SACNN/confusion_matrix_ResNet50_lab_saved_adam_best_model.png\", bbox_inches = 'tight', dpi = 600)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(Y_true, Y_pred_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(Y_true, Y_pred_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
